{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/logo.png\" style=\"width: 100px;\"/>\n",
    "<h1><center>Assignment 7</center></h1>\n",
    "<h3><center>First graded bonus assignment</center></h3>\n",
    "\n",
    "<center>Due: 23.12.2021 at 23:59</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to upload:\n",
    "\n",
    "Upload your solution via the VC course. Please upload **one Zip archive** per group. The Zip must contain:\n",
    "* Your solution **notebook** (a **.ipynb** file)\n",
    "* A **data folder** with the datasets (you probably don't have to change anything here)\n",
    "\n",
    "Your Zip should be named after the following scheme:\n",
    "\n",
    "* \"**yourname**\"\\_assignment\\_\"**number**\"\\_submission.zip\n",
    "\n",
    "\n",
    "### Please use coding comments!!!\n",
    "### 60 Points in total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) Random Forest (14 points)\n",
    "\n",
    "You have already worked with decision trees throughout the assignments. This time, instead of implementing algorithms yourself, you are going to use the scikit-learn library to process the data and generate decision trees.\n",
    "\n",
    "First of all, have a look at the data. Our target will be to determine, based on certain social factors, if a person makes more than $50K a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>?</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>?</td>\n",
       "      <td>10</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  education.num marital.status         occupation  \\\n",
       "0   90         ?              9        Widowed                  ?   \n",
       "1   82   Private              9        Widowed    Exec-managerial   \n",
       "2   66         ?             10        Widowed                  ?   \n",
       "3   54   Private              4       Divorced  Machine-op-inspct   \n",
       "4   41   Private             10      Separated     Prof-specialty   \n",
       "\n",
       "    relationship   race     sex  capital.gain  capital.loss  hours.per.week  \\\n",
       "0  Not-in-family  White  Female             0          4356              40   \n",
       "1  Not-in-family  White  Female             0          4356              18   \n",
       "2      Unmarried  Black  Female             0          4356              40   \n",
       "3      Unmarried  White  Female             0          3900              40   \n",
       "4      Own-child  White  Female             0          3900              40   \n",
       "\n",
       "  income  \n",
       "0  <=50K  \n",
       "1  <=50K  \n",
       "2  <=50K  \n",
       "3  <=50K  \n",
       "4  <=50K  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/adult.csv\")\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has to be preprocessed in several ways. First, you can see that there are a few cells containing a `?`, which means that this value is unknown. \n",
    "\n",
    "**Task 1 (1 point):**\n",
    "   1. Find out how many rows contain an unknown value\n",
    "   2. Remove these rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Row:  1843\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>45</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>6</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  education.num marital.status         occupation  \\\n",
       "1   82   Private              9        Widowed    Exec-managerial   \n",
       "3   54   Private              4       Divorced  Machine-op-inspct   \n",
       "4   41   Private             10      Separated     Prof-specialty   \n",
       "5   34   Private              9       Divorced      Other-service   \n",
       "6   38   Private              6      Separated       Adm-clerical   \n",
       "\n",
       "    relationship   race     sex  capital.gain  capital.loss  hours.per.week  \\\n",
       "1  Not-in-family  White  Female             0          4356              18   \n",
       "3      Unmarried  White  Female             0          3900              40   \n",
       "4      Own-child  White  Female             0          3900              40   \n",
       "5      Unmarried  White  Female             0          3770              45   \n",
       "6      Unmarried  White    Male             0          3770              40   \n",
       "\n",
       "  income  \n",
       "1  <=50K  \n",
       "3  <=50K  \n",
       "4  <=50K  \n",
       "5  <=50K  \n",
       "6  <=50K  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your solution here\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#count12 = df.shape[0]\n",
    "#print(count12)\n",
    "\n",
    "df2 = df.replace('?', np.NaN)\n",
    "\n",
    "#count = df2.isna().sum().sum()\n",
    "#print(count)\n",
    "countrow = (df2.isna().sum(axis=1) > 0).sum()\n",
    "print(\"Count Row: \", countrow)\n",
    "\n",
    "#filtered_df = df[df2.notnull()]\n",
    "df2 = df2.dropna()\n",
    "df2.head()\n",
    "#count11 = df2.shape[0]\n",
    "#print(count11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (1 point):** Next, split off the target attribute so that we have to variables `X` and `y` instead of just one dataframe. Also, re-encode `y` so that it has the values `0` and `1` instead of `<=50K` and `>50K`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "6    0\n",
       "Name: income, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_attribute = 'income'\n",
    "\n",
    "y_data = df2[target_attribute]\n",
    "x_data = df2.drop(target_attribute, axis=1)\n",
    "\n",
    "feature_names = x_data.columns\n",
    "class_names = y_data.name\n",
    "\n",
    "y_data = y_data.eq('>50K').mul(1)\n",
    "\n",
    "y_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, you could now use this data and train a decision tree on it with the ID3-algorithm. Unfortunately, the decision tree algorithms in scikit-learn only works with numerical features, and as you can see from the data inspection, there are still quite a few string features in there. So we need to do more preprocessing.\n",
    "\n",
    "**Task 3 (1 point):** Perform One-Hot-Encoding on the non-numerical attributes. *Hint: When you use a built-in function from pandas, you can do that in one line.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'categorical_columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-d07f671a37df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx_data2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mx_data2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'categorical_columns' is not defined"
     ]
    }
   ],
   "source": [
    "#Your solution here\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_columns = [\n",
    "    \"workclass\", \"marital.status\", \"occupation\",\n",
    "    \"relationship\", \"race\", \"sex\", \"capital.loss\", \"PAY_5\", \"PAY_6\"\n",
    "]\n",
    "# List of all numerical features\n",
    "numerical_columns = [\n",
    "    \"age\", \"education.num\",\n",
    "    \"capital.gain\", \"capital.loss\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\",\n",
    "    \"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\"\n",
    "]\n",
    "\n",
    "x_data2 = pd.get_dummies(x_data, columns=categorical_columns, drop_first=True)\n",
    "x_data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (2 points):** Look at the result from the One-Hot-Encoding. Answer the following questions.\n",
    "1. What does One-Hot-Encoding do?\n",
    "2. When you look a bit into the preprocessing-package from scikit-learn, you see that there are other encoders, such as the [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html). If you'd use that to transform the data, it would just replace each string value with a numerical value, and your dataframenow would not have as many columns as id does now. What is the problem with the LabelEncoder so that we decided not to use it here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your solution here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to do before training is to split the dataset into a training- and a test set. Maybe you find a function from scikit-learn that will do that for you.\n",
    "\n",
    "**Task 5 (1 point):** Use a function fom scikit-learn to split the dataset into training- and test data. The relation should be 70:30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please use this random seed whenever you use a function from sklearn that allows it\n",
    "RANDOM_SEEED = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is time to do the training! Start with a simple decision tree.\n",
    "\n",
    "**Task 6 (5 points):**    \n",
    "A)\n",
    "   1. Use sklearn's [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) to train a decision tree. Use the entropy as a criterion for splitting. Remember the random seed.\n",
    "   2. Calculate accuracy and F1-score on both the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) Do the same training again, this time using a Random Forest Classifier instead of a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7 (3 points):** Answer the following questions. As always, please justify your answers.\n",
    "   1. Which of the two classifiers you trained performs better?\n",
    "   2. What does a random forest do compared to a single decision tree?\n",
    "   3. Is the classifier overfitted to the training data? If so, how can you adjust the training to prevent that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your solution here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Deep Learning (23 points)\n",
    "\n",
    "We now want to train a Deep Learning model on an image classification task. For this we will use the framework *Keras*. Keras is a high-level neural network framework that allows straight-forward implementation and experimentation with networks that contain arbitrary layers. \n",
    "\n",
    "The framework is embedded in Google's efficient computation framework TensorFlow. So in order to use it, you have to install TensorFlow in the same conda environment you use to open this notebook. You can install it using the command __conda install tensorflow__. If you feel adventurous and have a GPU, you can also install a TensorFlow version with GPU support. For more information on that, see [here](https://www.tensorflow.org/install/gpu).\n",
    "\n",
    "__Note that GPU support is NOT necessary to solve this task.__ The setup for CUDA and similar libraries can be complicated and lead to very device-specific problems. If you already have CUDA installed and you are getting errors, see the tips and tricks section point 2), there is one common issue we can provide a fix for. \n",
    "\n",
    "After installation you can get familiar with the overall workflow in Keras by\n",
    "reading [this ressource](https://keras.io/guides/sequential_model/).\n",
    "\n",
    "Feel free to use other tutorials or installation guides. There are many high quality guides freely available online, which contain specific instructions and walkthroughs for your setup and OS. Please contact us if you need any help with that.\n",
    "\n",
    "We will implement a rather simple convolutional neural network that tackles the task of classifying input images into pre-defined classes. The dataset we\n",
    "are going to use for this assignment is called *Fashion-MNIST*. It consists of\n",
    "low-resolution images of 10 clothing categories.\n",
    "\n",
    "Let us first obtain the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Label   | Description |\n",
    "|:-------:|:------------|\n",
    "| 0 | T-Shirt/Top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1 (1 point):__ Find out the dimensions (width and height) of the images. Show the last image of the train set and determine its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2 (2 points):__ First of all we need to pre-process the images in a way they can be fed into a Convolutional Neural Network. It is important to think about the dimensions of the images: We've already explored width and height of the images but we also need to consider the color channel. Since the images of the dataset are all grayscale, a single image only has one value for every pixel in its representing 2D array. For Keras this dimensional expansion must be made explicit so that it can incorporate a single color channel. For this purpose the 2D array of an image has to be transformed into a 3D array (height x width x color). Perform this dimension expansion for your training and test images.\n",
    "\n",
    "Additionally a CNN performs better with small floating point values for the pixel data. Therefore make sure that all your pixel values are normalized to fall in the range between 0.0 and 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3 (1 point):__ When looking at the labels of the dataset it is noticeable that they are categorical. In statistics, categorical features are referred to as nominally scaled data. Explain the difference between the nominal scale and ordinal scale. Explain why we argue that the labels are nominally scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your solution here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 4 (1 point):__ Now transform the labels. *Hint: The __utils__ package of Keras will help you greatly with that.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 5 (2 points):__ Split the test set into test and validation set using a `test_size` of 0.5. \n",
    "What is the validation set for? Explain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your solution here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 6 (4 points):__ Let us now implement our own Convolutional Neural Network architecture with Keras. Doing so we will start with defining the CNN. After that we can compile it and finally fit it. Define the network's architecture and print out the model summary. The structure has to be the following:\n",
    "* Input layer that allows for the input of images preprocessed like above\n",
    "* Convolution layer with 32 filters and a 3x3 weight kernel. Activation: ReLU\n",
    "* Maxpooling layer using a 2x2 window\n",
    "* Convolution layer with 64 filters and a 3x3 weight kernel. Activation: ReLU\n",
    "* Maxpooling layer using a 2x2 window\n",
    "* Flatten Layer\n",
    "* Dense layer with 100 neurons and ReLU activation function\n",
    "* Output layer with the number of output classes and Softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 7 (2 points):__ Explain what a MaxPoolingLayer does and what the `pool_size` is for. Why do you need Pooling Layers in a CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your solution here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 8 (1 point):__ TensorBoard is a helpful tool that visualizes the inner workings of our CNN. Have a look at this link to get to know more about it: [here](https://www.tensorflow.org/tensorboard/). \n",
    "Initialize a TensorBoard. For `log_dir` choose a name for the directory where the log entries will later be saved in. The `histogram_freq` should be set to 1.  You will later use it when fitting the CNN. *Hint: Remember the name you chose for your log directory.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 9 (2 points):__ Let's compile our CNN so that the model is trainable by Keras! During compilation, several hyperparameters will also be given by the user. Compile your model to be trained with the optimizer strategy *Adam*, which is a variation of *Stochastic Gradient Descent*. For *Adam* please use a learning rate of 0.001. For the loss function please use the `categorical_entropy`. For metrics use the `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 10 (2 points):__ Now we can finally train our model. Train the model for 10 `epochs` with a `batch_size` of 32 with the training data and validate it simultaneously with our validation data with Keras. This can take some time. Don't forget to include the `TensorBoard` you initialized before. *Hint: By setting verbose to 1 you can watch the progress.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 11 (2 points):__ Make use of the TensorBoard you already included above and explore it. Doing so you have to open a terminal in the directory you run this notebook in. Include an image of the main graph that shows the architecture of your CNN as well as an image of the plotted epoch's accuracies. *Hint: You can clear the logs of previous runs: `rm -rf ./theNameYouChoseForLogdir`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your solution here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 12 (3 points):__ Finally, let us make use of the left out test data and evaluate our model again on the test data. Is there a sign of overfitting with respect to the test data? *Hint: Your TensorBoard can help you answering this question.* \n",
    "\n",
    "What is a common method to prevent a CNN from Overfitting? How is it done in Keras? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Your solution here..* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) Word embeddings and Text processing (23 points)\n",
    "In this part, we will take a look at word embeddings and text processing - very important concepts when it comes to natural language processing (NLP) which is a big part of machine learning.   \n",
    "We will start slowly with some basics. The goal behind this task is to give you an outlook on what else can be possible.\n",
    "\n",
    "To do this, we need to download and import a new package called _gensim_. Be sure to install the newest version. _Gensim_ is a powerful text-processing library with a repository of pre-trained models, which we can use.\n",
    "\n",
    "Be sure to use built-in functions for this task. For this you will have to find the documentation for gensim on your own and research what kind of functions will be helpful to solve the tasks.\n",
    "\n",
    "__Troubleshooting__: As _gensim_ can download text corpora and text processing models from their repository through a function call, an error could occur when you try to import it. To fix that you can manually install the library _smart_open_ at version _2.0.0_. This should resolve the problem.\n",
    "- conda install smart_open==2.0.0\n",
    "- pip install smart_open==2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1 (2 points):__ Research on your own, what __Word Embeddings__ are and what `Word2Vec` has to do with that to get a good idea of what we'll be dealing with in the next few tasks. Briefly explain what word embeddings are in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your solution here ...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will download a Word2Vec model named [_glove_](https://github.com/RaRe-Technologies/gensim-data), which was trained on part with the wikipedia text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this code does not work, try installing  smart_open==2.0.0\n",
    "import gensim.downloader as api\n",
    "word2vec = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2 (2 points)__:    \n",
    "1) What is the word embedding for the word *fairy*? Print out the array.   \n",
    "2) Give the most similar words to *fairy* and the same for *dwarf*? Give a short interpretation whether the result makes sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3 (2 points)__: Give the most similar words to *fairy* and the same for *dwarf*. Give a short interpretation whether the result makes sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (1 point):** Now let's find out how similar certain words are. What is a good metric to determine the similarity of words in the vector representation when you focus on the direction of the vectors rather than their length? Briefly explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your solution here ...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (2 points):** Now use the metric you described above to calculate the similarity between *fairy* and *dwarf*. Can you find a word that is more similar to fairy than dwarf (which is not in the list of most similar words)? Can you find a word that is less similar to fairy than dwarf? Give us the words and the corresponding similarity scores. Interpret the result.\n",
    "\n",
    "*Hint: See if you can find a built-in function to do the similarity calculation for you.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6 (3 points):** One advantage of context-based word embeddings is the fact that you can do basic arithmetic operations on them like summation or subtraction and still achieving \"somewhat\" sound results. Try this out by adding the word \"good\" to \"fairy\" and thus recieving a word embedding for a \"good fairy\". Print the resulting word embedding.\n",
    "\n",
    "Second: Find out what words are most similar to a \"good fairy\". Do you find the results sensible? Explain very briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now you used a pretrained word embedding. With gensim it is pretty easy to train your own word embeddings! You will now use your knowledge to train your own models. We give you a little head start with the correct packages and reading in, processing and tokenizing the text.\n",
    "\n",
    "We use a fairytale text but if you like you can fiddle around with your own. The fairytale text was taken from Project Guttenberg and is a collection of fairy tales by the Grimm Brothers. https://www.gutenberg.org/ebooks/2591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/biancazimmer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Python program to generate word vectors using Word2Vec\n",
    "\n",
    "# importing all necessary modules\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads ‘fairytale.txt’ file\n",
    "sample = open(\"data/fairytales.txt\", \"r\")\n",
    "s = sample.read()\n",
    "\n",
    "# Replaces escape character with space\n",
    "f = s.replace(\"\\n\", \" \")\n",
    "\n",
    "data = []\n",
    "\n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(f):\n",
    "    temp = []\n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "    data.append(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7 (1 point):** In the code snippet above we tokenized our text. Research what a tokenizer does in the context of NLP and describe this briefly in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your solution here ...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8 (2 points):** There is more than one way to train a word embedding. Research what the method \"Continuous Bag of Words\" and \"Skip Gram\" mean, how they work and what their difference is. Explain this in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your solution here ...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 9 (2 points):**\n",
    "\n",
    "Now we will use your new found knowledge to first train a CBOW model with the fairytale data. Use a gensim built-in function to train a Word2Vec word embedding with the CBOW method. For the first one try use a window of 5, a size of 100, a minimum count of 1 and train it for 10 epochs.\n",
    "\n",
    "Now use this model to calculate the similarity between \"fairy\" and \"good\" as well as \"fairy\" and \"dwarf\". Interpret the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CBOW model\n",
    "\n",
    "# Print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 10 (2 points):**\n",
    "\n",
    "Now again use a gensim built-in function to train a Word2Vec word embedding with the Skip Gram method (*Hint: If you chose the right function above there is only one hyperparameter you have to add*). For your first try use a window of 5, a size of 100, a minimum count of 1 and train it for 10 epochs.\n",
    "\n",
    "Now use this second model to calculate the similarity between \"fairy\" and \"good\" as well as \"fairy\" and \"dwarf\". Interpret the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Skip Gram model\n",
    "\n",
    "# Print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 11 (1 point):** Are you surprised that the similarity scores of the two models differ? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your solution here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 12 (3 points):** Now that you have played around with word embeddings a little bit and got to know the theory behind them. What do you think would be good practical examples to use them in the machine learning context. Name three and explain briefly what these examples are and why you think that would be a good use case. Feel free to use the internet for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your solution here *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips and Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) CUDA issues\n",
    "One rather common error message concerns a problem with 'cudnn', and a message that the convolution could not be found: \"Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\"   \n",
    "In that case, copy the following code, insert it in the first code field after the description of task 3 begins, remove the '*#*' from all lines, and execute that code field again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.compat.v1 import ConfigProto\n",
    "#from tensorflow.compat.v1 import InteractiveSession\n",
    "#config = ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "#session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using CUDA with an NVidia RTX 3000-Series GPU, pay very close attention to the \"Hardware Requirements\" points on [here](https://www.tensorflow.org/install/gpu). Also notice which Tensorflow version you are using, as the older ones may not be compatible with you new GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Embedding images\n",
    "You can embed images in a jupyter notebook on two ways: <br/>\n",
    "First, you can use the IPython kernel to draw an image everytime the code cell is run like shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/logo.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, you can embed images directly in a Markdown cell as shown below. You can either use markdown syntax or write plain HTML code. Sometimes HTML code is more practical, as you have much finer control over the HTML elements.\n",
    "\n",
    "1. Markdown syntax:\n",
    "![title](images/logo.png)\n",
    "2. HTML syntax\n",
    "<img src=\"images/logo.png\" style=\"width: 70px;\"/>\n",
    "\n",
    "If you are having trouble with **markdown images not refreshing after you change them on disk** you need to refresh your browser. The browser chaches images and the old image is still in the cache."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
